{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPGldR6jf+nD0rlC81wuxi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eva-sarin/NLP-BASICS/blob/master/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Je0NKy3CHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSqUocuZKcDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RhS2TKx8iM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d76d2743-4600-4bbc-8c61-bad9fb5c9724"
      },
      "source": [
        "from lxml import html\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import html.parser as htmlparser\n",
        "import re\n",
        "import urllib.request\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "url='https://www.gutenberg.org/ebooks/1727'\n",
        "mainWebsite=\"https://www.gutenberg.org/\"\n",
        "\n",
        "page = requests.get(url)\n",
        "tree = html.fromstring(page.content)\n",
        "\n",
        "buyers = tree.xpath('//a[@title=\"Download\"]/text()')\n",
        "\n",
        "prices = tree.xpath('//a[@charset=\"utf-8\"]/text()')\n",
        "\n",
        "if 'Plain Text UTF-8' in prices:\n",
        "    \n",
        "    #soup = BeautifulSoup(r, \"html.parser\")\n",
        "    print(url)\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    #open('temp.txt', 'w').write(r.text)\n",
        "    #print(r.text)\n",
        "    \n",
        "    links = re.findall('a href=\"(.*txt)\"', r.text)\n",
        "    print(links)\n",
        "    for l in links:\n",
        "    \n",
        "        fileurl= mainWebsite+l\n",
        "        print(fileurl)\n",
        "##            r2 = requests.get(fileurl, allow_redirects=True)\n",
        "        with urllib.request.urlopen(fileurl) as response, open('trainingtext.txt', 'wb') as out_file:\n",
        "            shutil.copyfileobj(response, out_file)##                print(f)\n",
        "##                html = f.read().decode('utf-8')\n",
        "##                print(html)\n",
        "##                open('trainingtext.txt', 'a+').write(html)\n",
        "            break\n",
        "            \n",
        "        \n",
        "##    cheese = tree.xpath('//a[@class=\"link\"]/text()')\n",
        "##    print(cheese)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.gutenberg.org/ebooks/1727\n",
            "['/files/1727/1727-0.txt']\n",
            "https://www.gutenberg.org//files/1727/1727-0.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-O_V1fbKdck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "a5df978d-84b9-4c7f-a83e-a8a48995fb6f"
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "sess=gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess,model_name='124M')\n",
        "\n",
        "gpt2.generate(sess,model_name='124M')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-afc8e858c26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgpt_2_simple\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'124M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_2_simple'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6-f1bvEJEX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "38f0886c-f413-459a-a40a-5391cdecb4a0"
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import sys\n",
        "import shutil\n",
        "import re\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "from tensorflow.python.client import device_lib\n",
        "import time\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import argparse\n",
        "\n",
        "# if in Google Colaboratory\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
        "from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
        "from gpt_2_simple.src.accumulate import AccumulatingOptimizer\n",
        "\n",
        "assert tf.__version__ < '2.0.0', \"gpt-2-simple currently does not support \" \\\n",
        "    \"TensorFlow 2.0. You'll need to use a virtualenv/cloud computer which \" \\\n",
        "    \"has Tensorflow 1.X on it.\"\n",
        "\n",
        "\n",
        "def download_file_with_progress(url_base, sub_dir, model_name, file_name):\n",
        "    \"\"\"General utility for incrementally downloading files from the internet\n",
        "    with progress bar\n",
        "    from url_base / sub_dir / filename\n",
        "    to local file system sub_dir / filename\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : str\n",
        "        name of file to get e.g. \"hparams.json\"\n",
        "    sub_dir: str\n",
        "        subdirectory inside which to get and copy locally eg. \"models/124M\" \n",
        "        no trailing slash\n",
        "    url_base : str\n",
        "        Start of URL location specifying server and any base directories no \n",
        "        trailing slash\n",
        "        e.g. \"https://storage.googleapis.com/gpt-2\"\n",
        "    \"\"\"\n",
        "\n",
        "    # set to download 1MB at a time. This could be much larger with no issue\n",
        "    DOWNLOAD_CHUNK_SIZE = 1024 * 1024\n",
        "    r = requests.get(url_base + \"/models/\" + model_name + \"/\" + file_name, stream=True)\n",
        "    with open(os.path.join(sub_dir, file_name), 'wb') as f:\n",
        "        file_size = int(r.headers[\"content-length\"])\n",
        "        with tqdm(ncols=100, desc=\"Fetching \" + file_name,\n",
        "                  total=file_size, unit_scale=True) as pbar:\n",
        "            for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n",
        "                f.write(chunk)\n",
        "                pbar.update(DOWNLOAD_CHUNK_SIZE)\n",
        "   \n",
        "\n",
        "def download_gpt2(model_dir='models', model_name='124M'):\n",
        "    \"\"\"Downloads the GPT-2 model into the current directory\n",
        "    from Google Cloud Storage.\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_dir : str\n",
        "        parent directory of model to download\n",
        "    model_name : str\n",
        "        name of the GPT-2 model to download. \n",
        "        As of 22 May 2019 one of \"124M\" or \"355M\" but may later include other \n",
        "        model sizes\n",
        "    Adapted from https://github.com/openai/gpt-2/blob/master/download_model.py\n",
        "    \"\"\"\n",
        "\n",
        "    # create the <model_dir>/<model_name> subdirectory if not present\n",
        "    sub_dir = os.path.join(model_dir, model_name)\n",
        "    if not os.path.exists(sub_dir):\n",
        "        os.makedirs(sub_dir)\n",
        "    sub_dir = sub_dir.replace('\\\\', '/')  # needed for Windows\n",
        "\n",
        "    for file_name in ['checkpoint', 'encoder.json', 'hparams.json',\n",
        "                      'model.ckpt.data-00000-of-00001', 'model.ckpt.index',\n",
        "                      'model.ckpt.meta', 'vocab.bpe']:\n",
        "        download_file_with_progress(url_base=\"https://storage.googleapis.com/gpt-2\",\n",
        "                                    sub_dir=sub_dir,\n",
        "                                    model_name=model_name,\n",
        "                                    file_name=file_name)\n",
        "\n",
        "\n",
        "def start_tf_sess(threads=-1, server=None):\n",
        "    \"\"\"\n",
        "    Returns a tf.Session w/ config\n",
        "    \"\"\"\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
        "    if threads > 0:\n",
        "        config.intra_op_parallelism_threads = threads\n",
        "        config.inter_op_parallelism_threads = threads\n",
        "\n",
        "    if server is not None:\n",
        "        return tf.compat.v1.Session(target=server.target, config=config)\n",
        "    \n",
        "    return tf.compat.v1.Session(config=config)\n",
        "\n",
        "\n",
        "def reset_session(sess, threads=-1, server=None):\n",
        "    \"\"\"Resets the current TensorFlow session, to clear memory\n",
        "    or load another model.\n",
        "    \"\"\"\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    sess.close()\n",
        "    sess = start_tf_sess(threads, server)\n",
        "    return sess\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "\n",
        "def finetune(sess,\n",
        "             dataset,\n",
        "             steps=-1,\n",
        "             model_name='124M',\n",
        "             model_dir='models',\n",
        "             combine=50000,\n",
        "             batch_size=1,\n",
        "             learning_rate=0.0001,\n",
        "             accumulate_gradients=5,\n",
        "             restore_from='latest',\n",
        "             run_name='run1',\n",
        "             checkpoint_dir='checkpoint',\n",
        "             sample_every=100,\n",
        "             sample_length=1023,\n",
        "             sample_num=1,\n",
        "             multi_gpu=False,\n",
        "             save_every=1000,\n",
        "             print_every=1,\n",
        "             max_checkpoints=1,\n",
        "             use_memory_saving_gradients=False,\n",
        "             only_train_transformer_layers=False,\n",
        "             optimizer='adam',\n",
        "             overwrite=False):\n",
        "    \"\"\"Finetunes the model on the given dataset.\n",
        "    Adapted from https://github.com/nshepperd/gpt-2/blob/finetuning/train.py.\n",
        "    See that file for parameter definitions.\n",
        "    \"\"\"\n",
        "\n",
        "    # assert model_name not in ['774M', '1558M'] or multi_gpu, \"Currently, a modern single GPU cannot finetune the 774M GPT-2 model or larger.\"\n",
        "\n",
        "    SAMPLE_DIR = 'samples'\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
        "\n",
        "    def maketree(path):\n",
        "        try:\n",
        "            os.makedirs(path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    maketree(checkpoint_path)\n",
        "    files = [f for f in os.listdir(checkpoint_path)]\n",
        "    for file in ['hparams.json', 'encoder.json', 'vocab.bpe']:\n",
        "        try:\n",
        "            shutil.copyfile(os.path.join(model_dir, model_name, file),\n",
        "                            os.path.join(checkpoint_path, file))\n",
        "        except FileNotFoundError as fnf_error:\n",
        "            print(\"You need to download the GPT-2 model first via download_gpt2()\")\n",
        "            raise(fnf_error)\n",
        "\n",
        "    enc = encoder.get_encoder(checkpoint_path)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if sample_length > hparams.n_ctx:\n",
        "        raise ValueError(\n",
        "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    if model_name not in ['117M', '124M']:\n",
        "        use_memory_saving_gradients = True\n",
        "        only_train_transformer_layers = True\n",
        "        accumulate_gradients = 1\n",
        "\n",
        "    context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
        "    gpus = []\n",
        "\n",
        "    if multi_gpu:\n",
        "        gpus = get_available_gpus()\n",
        "\n",
        "    output = model.model(hparams=hparams, X=context, gpus=gpus)\n",
        "    loss = tf.reduce_mean(\n",
        "        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
        "\n",
        "    tf_sample = sample.sample_sequence(\n",
        "        hparams=hparams,\n",
        "        length=sample_length,\n",
        "        context=context,\n",
        "        batch_size=batch_size,\n",
        "        temperature=1.0,\n",
        "        top_k=40)\n",
        "\n",
        "    all_vars = [v for v in tf.compat.v1.trainable_variables() if 'model' in v.name]\n",
        "    train_vars = [v for v in all_vars if '/h' in v.name] if only_train_transformer_layers else all_vars\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "    if accumulate_gradients > 1:\n",
        "        if use_memory_saving_gradients:\n",
        "            exit(\"Memory saving gradients are not implemented for gradient accumulation yet.\")\n",
        "        opt = AccumulatingOptimizer(\n",
        "            opt=opt,\n",
        "            var_list=train_vars)\n",
        "        opt_reset = opt.reset()\n",
        "        opt_compute = opt.compute_gradients(loss)\n",
        "        opt_apply = opt.apply_gradients()\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', opt_apply)\n",
        "    else:\n",
        "        if use_memory_saving_gradients:\n",
        "            opt_grads = memory_saving_gradients.gradients(loss, train_vars)\n",
        "        else:\n",
        "            opt_grads = tf.gradients(ys=loss, xs=train_vars)\n",
        "        opt_grads = list(zip(opt_grads, train_vars))\n",
        "        opt_apply = opt.apply_gradients(opt_grads)\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', loss)\n",
        "\n",
        "    summary_log = tf.compat.v1.summary.FileWriter(checkpoint_path)\n",
        "\n",
        "    saver = tf.compat.v1.train.Saver(\n",
        "        var_list=all_vars,\n",
        "        max_to_keep=max_checkpoints)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    if restore_from == 'latest':\n",
        "        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n",
        "        if ckpt is None:\n",
        "            # Get fresh GPT weights if new run.\n",
        "            ckpt = tf.train.latest_checkpoint(\n",
        "                os.path.join(model_dir, model_name))\n",
        "    elif restore_from == 'fresh':\n",
        "        ckpt = tf.train.latest_checkpoint(\n",
        "            os.path.join(model_dir, model_name))\n",
        "    else:\n",
        "        ckpt = tf.train.latest_checkpoint(restore_from)\n",
        "    print('Loading checkpoint', ckpt)\n",
        "    saver.restore(sess, ckpt)\n",
        "\n",
        "    print('Loading dataset...')\n",
        "    chunks = load_dataset(enc, dataset, combine)\n",
        "    data_sampler = Sampler(chunks)\n",
        "    print('dataset has', data_sampler.total_size, 'tokens')\n",
        "    print('Training...')\n",
        "\n",
        "    counter = 1\n",
        "    counter_path = os.path.join(checkpoint_path, 'counter')\n",
        "    if os.path.exists(counter_path) and restore_from == 'latest':\n",
        "        # Load the step number if we're resuming a run\n",
        "        # Add 1 so we don't immediately try to save again\n",
        "        with open(counter_path, 'r') as fp:\n",
        "            counter = int(fp.read()) + 1\n",
        "    counter_base = counter\n",
        "\n",
        "    def save():\n",
        "        maketree(checkpoint_path)\n",
        "        print(\n",
        "            'Saving',\n",
        "            os.path.join(checkpoint_path,\n",
        "                         'model-{}').format(counter-1))\n",
        "        saver.save(\n",
        "            sess,\n",
        "            os.path.join(checkpoint_path, 'model'),\n",
        "            global_step=counter-1)\n",
        "        with open(counter_path, 'w') as fp:\n",
        "            fp.write(str(counter-1) + '\\n')\n",
        "\n",
        "    def generate_samples():\n",
        "        context_tokens = data_sampler.sample(1)\n",
        "        all_text = []\n",
        "        index = 0\n",
        "        while index < sample_num:\n",
        "            out = sess.run(\n",
        "                tf_sample,\n",
        "                feed_dict={context: batch_size * [context_tokens]})\n",
        "            for i in range(min(sample_num - index, batch_size)):\n",
        "                text = enc.decode(out[i])\n",
        "                text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
        "                    index + 1, text)\n",
        "                all_text.append(text)\n",
        "                index += 1\n",
        "        print(text)\n",
        "        maketree(os.path.join(SAMPLE_DIR, run_name))\n",
        "        with open(\n",
        "                os.path.join(SAMPLE_DIR, run_name,\n",
        "                             'samples-{}').format(counter), 'w') as fp:\n",
        "            fp.write('\\n'.join(all_text))\n",
        "\n",
        "    def sample_batch():\n",
        "        return [data_sampler.sample(1024) for _ in range(batch_size)]\n",
        "\n",
        "    if overwrite and restore_from == 'latest':\n",
        "        for file in files:\n",
        "            if file.startswith('model') or file.startswith('events'):\n",
        "                os.remove(os.path.join(checkpoint_path, file))\n",
        "        save()\n",
        "\n",
        "    avg_loss = (0.0, 0.0)\n",
        "    start_time = time.time()\n",
        "\n",
        "    if steps:\n",
        "        steps = int(steps)\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            if steps > 0 and counter == (counter_base + steps):\n",
        "                save()\n",
        "                return\n",
        "            if (counter - 1) % save_every == 0 and counter > 1:\n",
        "                save()\n",
        "            if (counter - 1) % sample_every == 0 and counter > 1:\n",
        "                generate_samples()\n",
        "\n",
        "            if accumulate_gradients > 1:\n",
        "                sess.run(opt_reset)\n",
        "                for _ in range(accumulate_gradients):\n",
        "                    sess.run(\n",
        "                        opt_compute, feed_dict={context: sample_batch()})\n",
        "                (v_loss, v_summary) = sess.run((opt_apply, summary_loss))\n",
        "            else:\n",
        "                (_, v_loss, v_summary) = sess.run(\n",
        "                    (opt_apply, loss, summary_loss),\n",
        "                    feed_dict={context: sample_batch()})\n",
        "\n",
        "            summary_log.add_summary(v_summary, counter)\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
        "                            avg_loss[1] * 0.99 + 1.0)\n",
        "\n",
        "                print(\n",
        "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
        "                    .format(\n",
        "                        counter=counter,\n",
        "                        time=time.time() - start_time,\n",
        "                        loss=v_loss,\n",
        "                        avg=avg_loss[0] / avg_loss[1]))\n",
        "\n",
        "            counter += 1\n",
        "    except KeyboardInterrupt:\n",
        "        print('interrupted')\n",
        "        save()\n",
        "\n",
        "\n",
        "def load_gpt2(sess,\n",
        "              checkpoint='latest',\n",
        "              run_name=\"run1\",\n",
        "              checkpoint_dir=\"checkpoint\",\n",
        "              model_name=None,\n",
        "              model_dir='models',\n",
        "              multi_gpu=False):\n",
        "    \"\"\"Loads the model checkpoint or existing model into a TensorFlow session\n",
        "    for repeated predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    if model_name:\n",
        "        checkpoint_path = os.path.join(model_dir, model_name)\n",
        "    else:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
        "\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    context = tf.compat.v1.placeholder(tf.int32, [1, None])\n",
        "\n",
        "    gpus = []\n",
        "    if multi_gpu:\n",
        "        gpus = get_available_gpus()\n",
        "\n",
        "    output = model.model(hparams=hparams, X=context, gpus=gpus)\n",
        "\n",
        "    if checkpoint=='latest':\n",
        "        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n",
        "    else:\n",
        "        ckpt = os.path.join(checkpoint_path,checkpoint)\n",
        "\n",
        "    saver = tf.compat.v1.train.Saver(allow_empty=True)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    if model_name:\n",
        "        print('Loading pretrained model', ckpt)\n",
        "    else:\n",
        "        print('Loading checkpoint', ckpt)\n",
        "    saver.restore(sess, ckpt)\n",
        "\n",
        "\n",
        "def generate(sess,\n",
        "             run_name='run1',\n",
        "             checkpoint_dir='checkpoint',\n",
        "             model_name=None,\n",
        "             model_dir='models',\n",
        "             sample_dir='samples',\n",
        "             return_as_list=False,\n",
        "             truncate=None,\n",
        "             destination_path=None,\n",
        "             sample_delim='=' * 20 + '\\n',\n",
        "             prefix=None,\n",
        "             seed=None,\n",
        "             nsamples=1,\n",
        "             batch_size=1,\n",
        "             length=1023,\n",
        "             temperature=0.7,\n",
        "             top_k=0,\n",
        "             top_p=0.0,\n",
        "             include_prefix=True):\n",
        "    \"\"\"Generates text from a model loaded into memory.\n",
        "    Adapted from https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n",
        "    \"\"\"\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    if nsamples == 1:\n",
        "        sample_delim = ''\n",
        "\n",
        "    if prefix == '':\n",
        "        prefix = None\n",
        "\n",
        "    if model_name:\n",
        "        checkpoint_path = os.path.join(model_dir, model_name)\n",
        "    else:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
        "\n",
        "    enc = encoder.get_encoder(checkpoint_path)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if prefix:\n",
        "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
        "        context_tokens = enc.encode(prefix)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "    output = sample.sample_sequence(\n",
        "        hparams=hparams,\n",
        "        length=min(length, 1023 - (len(context_tokens) if prefix else 0)),\n",
        "        start_token=enc.encoder['<|endoftext|>'] if not prefix else None,\n",
        "        context=context if prefix else None,\n",
        "        batch_size=batch_size,\n",
        "        temperature=temperature, top_k=top_k, top_p=top_p\n",
        "    )[:, 1:]\n",
        "\n",
        "    if destination_path:\n",
        "        f = open(destination_path, 'w')\n",
        "    generated = 0\n",
        "    gen_texts = []\n",
        "    while generated < nsamples:\n",
        "        if not prefix:\n",
        "            out = sess.run(output)\n",
        "        else:\n",
        "            out = sess.run(output, feed_dict={\n",
        "                    context: batch_size * [context_tokens]\n",
        "                })\n",
        "        for i in range(batch_size):\n",
        "            generated += 1\n",
        "            gen_text = enc.decode(out[i])\n",
        "            if prefix:\n",
        "                gen_text = enc.decode(context_tokens[:1]) + gen_text\n",
        "            if truncate:\n",
        "                truncate_esc = re.escape(truncate)\n",
        "                if prefix and not include_prefix:\n",
        "                    prefix_esc = re.escape(prefix)\n",
        "                    pattern = '(?:{})(.*?)(?:{})'.format(prefix_esc,\n",
        "                                                         truncate_esc)\n",
        "                else:\n",
        "                    pattern = '(.*?)(?:{})'.format(truncate_esc)\n",
        "\n",
        "                trunc_text = re.search(pattern, gen_text, re.S)\n",
        "                if trunc_text:\n",
        "                    gen_text = trunc_text.group(1)\n",
        "            gen_text = gen_text.lstrip('\\n')\n",
        "            if destination_path:\n",
        "                f.write(\"{}\\n{}\".format(gen_text, sample_delim))\n",
        "            if not return_as_list and not destination_path:\n",
        "                print(\"{}\\n{}\".format(gen_text, sample_delim), end='')\n",
        "            gen_texts.append(gen_text)\n",
        "\n",
        "    if destination_path:\n",
        "        f.close()\n",
        "\n",
        "    if return_as_list:\n",
        "        return gen_texts\n",
        "\n",
        "\n",
        "def generate_to_file(sess,\n",
        "                     run_name='run1',\n",
        "                     checkpoint_dir='checkpoint',\n",
        "                     model_name=None,\n",
        "                     model_dir='models',\n",
        "                     truncate=None,\n",
        "                     destination_path='gpt_2_gen_texts.txt',\n",
        "                     sample_delim='=' * 20 + '\\n',\n",
        "                     prefix=None,\n",
        "                     seed=None,\n",
        "                     nsamples=1,\n",
        "                     batch_size=1,\n",
        "                     length=1023,\n",
        "                     temperature=0.7,\n",
        "                     top_k=0,\n",
        "                     top_p=0.0,\n",
        "                     include_prefix=True):\n",
        "    \"\"\"Generates the texts to a file.\n",
        "    sample_delim separates texts: set to '' if each text is a small document.\n",
        "    Adapted from https://github.com/minimaxir/textgenrnn/blob/master/textgenrnn/textgenrnn.py\n",
        "    \"\"\"\n",
        "\n",
        "    generate(sess=sess,\n",
        "             run_name=run_name,\n",
        "             checkpoint_dir=checkpoint_dir,\n",
        "             model_name=model_name,\n",
        "             model_dir=model_dir,\n",
        "             return_as_list=False,\n",
        "             truncate=truncate,\n",
        "             destination_path=destination_path,\n",
        "             sample_delim=sample_delim,\n",
        "             prefix=prefix,\n",
        "             seed=seed,\n",
        "             nsamples=nsamples,\n",
        "             batch_size=batch_size,\n",
        "             length=length,\n",
        "             temperature=temperature,\n",
        "             top_k=top_k,\n",
        "             top_p=top_p,\n",
        "             include_prefix=include_prefix)\n",
        "\n",
        "\n",
        "def mount_gdrive():\n",
        "    \"\"\"Mounts the user's Google Drive in Colaboratory.\"\"\"\n",
        "    assert 'google.colab' in sys.modules, \"You must be in Colaboratory to mount your Google Drive\"\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def is_mounted():\n",
        "    \"\"\"Checks if the Google Drive is mounted.\"\"\"\n",
        "    assert os.path.isdir('/content/drive'), \"You must mount first using mount_gdrive()\"\n",
        "\n",
        "\n",
        "def get_tarfile_name(checkpoint_folder):\n",
        "    \"\"\"Converts a folder path into a filename for a .tar archive\"\"\"\n",
        "    tarfile_name = checkpoint_folder.replace(os.path.sep, '_') + '.tar'\n",
        "\n",
        "    return tarfile_name\n",
        "\n",
        "\n",
        "def copy_checkpoint_to_gdrive(run_name='run1', copy_folder=False):\n",
        "    \"\"\"Copies the checkpoint folder to a mounted Google Drive.\"\"\"\n",
        "    is_mounted()\n",
        "\n",
        "    checkpoint_folder = os.path.join('checkpoint', run_name)\n",
        "\n",
        "    if copy_folder:\n",
        "        shutil.copytree(checkpoint_folder, \"/content/drive/My Drive/\" + checkpoint_folder)\n",
        "    else:\n",
        "        file_path = get_tarfile_name(checkpoint_folder)\n",
        "\n",
        "        # Reference: https://stackoverflow.com/a/17081026\n",
        "        with tarfile.open(file_path, 'w') as tar:\n",
        "            tar.add(checkpoint_folder)\n",
        "\n",
        "        shutil.copyfile(file_path, \"/content/drive/My Drive/\" + file_path)\n",
        "\n",
        "\n",
        "def copy_checkpoint_from_gdrive(run_name='run1', copy_folder=False):\n",
        "    \"\"\"Copies the checkpoint folder from a mounted Google Drive.\"\"\"\n",
        "    is_mounted()\n",
        "\n",
        "    checkpoint_folder = os.path.join('checkpoint', run_name)\n",
        "\n",
        "    if copy_folder:\n",
        "        shutil.copytree(\"/content/drive/My Drive/\" + checkpoint_folder, checkpoint_folder)\n",
        "    else:\n",
        "        file_path = get_tarfile_name(checkpoint_folder)\n",
        "\n",
        "        shutil.copyfile(\"/content/drive/My Drive/\" + file_path, file_path)\n",
        "\n",
        "        with tarfile.open(file_path, 'r') as tar:\n",
        "            tar.extractall()\n",
        "\n",
        "\n",
        "def copy_file_to_gdrive(file_path):\n",
        "    \"\"\"Copies a file to a mounted Google Drive.\"\"\"\n",
        "    is_mounted()\n",
        "\n",
        "    shutil.copyfile(file_path, \"/content/drive/My Drive/\" + file_path)\n",
        "\n",
        "\n",
        "def copy_file_from_gdrive(file_path):\n",
        "    \"\"\"Copies a file from a mounted Google Drive.\"\"\"\n",
        "    is_mounted()\n",
        "\n",
        "    shutil.copyfile(\"/content/drive/My Drive/\" + file_path, file_path)\n",
        "\n",
        "\n",
        "def is_gpt2_downloaded(model_dir='models', model_name='124M'):\n",
        "    \"\"\"Checks if the original model + associated files are present in folder.\"\"\"\n",
        "\n",
        "    for filename in ['checkpoint', 'encoder.json', 'hparams.json',\n",
        "                     'model.ckpt.data-00000-of-00001', 'model.ckpt.index',\n",
        "                     'model.ckpt.meta', 'vocab.bpe']:\n",
        "        if not os.path.isfile(os.path.join(model_dir, model_name, filename)):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def encode_csv(csv_path, out_path='csv_encoded.txt', header=True,\n",
        "               start_token=\"<|startoftext|>\",\n",
        "               end_token=\"<|endoftext|>\"):\n",
        "    \"\"\"Encodes a single-column CSV to a format suitable for gpt-2-simple.\n",
        "       Automatically adds the specified prefix and suffix tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(csv_path, 'r', encoding='utf8', errors='ignore') as f:\n",
        "        with open(out_path, 'w', encoding='utf8', errors='ignore') as w:\n",
        "            if header:\n",
        "                f.readline()\n",
        "            reader = csv.reader(f)\n",
        "            for row in reader:\n",
        "                w.write(start_token + row[0] + end_token + \"\\n\")\n",
        "\n",
        "\n",
        "def encode_dataset(file_path, model_dir='models', out_path='text_encoded.npz',\n",
        "                   model_name=\"124M\",\n",
        "                   combine=50000):\n",
        "    \"\"\"Preencodes a text document into chunks and compresses it,\n",
        "    saving time when generated.\n",
        "    Adapted from https://github.com/nshepperd/gpt-2/blob/finetuning/encode.py\n",
        "    \"\"\"\n",
        "\n",
        "    model_path = os.path.join(model_dir, model_name)\n",
        "    enc = encoder.get_encoder(model_path)\n",
        "    print('Reading files')\n",
        "    chunks = load_dataset(enc, file_path, combine)\n",
        "    print('Writing', out_path)\n",
        "    np.savez_compressed(out_path, *chunks)\n",
        "\n",
        "\n",
        "def cmd():\n",
        "    \"\"\"Function called when invoking from the terminal.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Easily retrain OpenAI's GPT-2 text-generating model on new texts. (https://github.com/minimaxir/gpt-2-simple)\"\n",
        "    )\n",
        "\n",
        "    # Explicit arguments\n",
        "    \n",
        "    parser.add_argument(\n",
        "        '--mode', help='Mode for using the CLI (either \"finetune\" or \"generate\") [Required]', nargs='?')\n",
        "    parser.add_argument(\n",
        "        '--run_name',  help=\"[finetune/generate] Run number to save/load the model\",\n",
        "        nargs='?', default='run1')\n",
        "    parser.add_argument(\n",
        "        '--checkpoint_dir', help=\"[finetune] Path of the checkpoint directory\",\n",
        "        nargs='?', default='checkpoint')\n",
        "    parser.add_argument(\n",
        "        '--model_name',  help=\"[finetune] Name of the GPT-2 model to finetune\",\n",
        "        nargs='?', default='124M')\n",
        "    parser.add_argument(\n",
        "        '--model_dir', help=\"[finetune] Path of directory of the GPT-2 model to finetune\",\n",
        "        nargs='?', default='models')\n",
        "    parser.add_argument(\n",
        "        '--dataset',  help=\"[finetune] Path to the source text.\",\n",
        "        nargs='?', default=None)\n",
        "    parser.add_argument(\n",
        "        '--steps',  help=\"[finetune] Number of steps to train (-1 for infinite)\",\n",
        "        nargs='?', default=-1)\n",
        "    parser.add_argument(\n",
        "        '--restore_from',  help=\"[finetune] Whether to load model 'fresh' or from 'latest' checkpoint.\",\n",
        "        nargs='?', default='latest')\n",
        "    parser.add_argument(\n",
        "        '--sample_every',  help=\"[finetune] After how many steps to print sample\",\n",
        "        nargs='?', default=1000000, type=int)\n",
        "    parser.add_argument(\n",
        "        '--save_every',  help=\"[finetune] After how many steps to save checkpoint\",\n",
        "        nargs='?', default=100, type=int)\n",
        "    parser.add_argument(\n",
        "        '--print_every',  help=\"[finetune] After how many steps to print progress\",\n",
        "        nargs='?', default=10, type=int)\n",
        "    parser.add_argument(\n",
        "        '--optimizer',  help=\"[finetune] Optimizer to use for finetuning (adam or sgd)\",\n",
        "        nargs='?', default='adam')\n",
        "    parser.add_argument(\n",
        "        '--overwrite',  help=\"[finetune] Overwrite existing model when continuing training\",\n",
        "        nargs='?', default=False, type=lambda x: (str(x).lower() == 'true'))\n",
        "    parser.add_argument(\n",
        "        '--nfiles',  help=\"[generate] How many files to generate.\",\n",
        "        nargs='?', default=1, type=int)\n",
        "    parser.add_argument(\n",
        "        '--nsamples',  help=\"[generate] How many texts to generate.\",\n",
        "        nargs='?', default=1, type=int)\n",
        "    parser.add_argument(\n",
        "        '--folder',  help=\"[generate] Folder to save the generated files\",\n",
        "        nargs='?', default=\"gen\", type=str)\n",
        "    parser.add_argument(\n",
        "        '--length',  help=\"[generate] Length (tokens) of the generated texts\",\n",
        "        nargs='?', default=1023, type=int)\n",
        "    parser.add_argument(\n",
        "        '--temperature',  help=\"[generate] Temperature of the generated texts\",\n",
        "        nargs='?', default=0.7, type=float)\n",
        "    parser.add_argument(\n",
        "        '--top_k',  help=\"[generate] Sample only from top k tokens\",\n",
        "        nargs='?', default=0, type=int)\n",
        "    parser.add_argument(\n",
        "        '--top_p',  help=\"[generate] Sample from top p prob (overrides top_k if nonzero)\",\n",
        "        nargs='?', default=0.0, type=float)\n",
        "    parser.add_argument(\n",
        "        '--batch_size',  help=\"[generate] Batch size for generation (increase for GPUs)\",\n",
        "        nargs='?', default=1, type=int)\n",
        "    parser.add_argument(\n",
        "        '--prefix',  help=\"[generate] Prefix for generated texts\",\n",
        "        nargs='?', default=None)\n",
        "    parser.add_argument(\n",
        "        '--truncate',  help=\"[generate] Truncation for generated texts\",\n",
        "        nargs='?', default=None)\n",
        "    # https://stackoverflow.com/a/46951029\n",
        "    parser.add_argument(\n",
        "        '--include_prefix',  help=\"[generate] Include prefix when truncating.\",\n",
        "        nargs='?', default=True, type=lambda x: (str(x).lower() == 'true'))\n",
        "    parser.add_argument(\n",
        "        '--sample_delim',  help=\"[generate] Delimiter between each generated sample.\",\n",
        "        nargs='?', default='=' * 20 + '\\n', type=str)\n",
        "    parser.add_argument(\n",
        "        '--multi_gpu',  help=\"[generate/finetune] Attempt to allocate multiple GPUs for running.\",\n",
        "        nargs='?', default=True, type=lambda x: (str(x).lower() == 'true'))\n",
        "\n",
        "    # Positional arguments\n",
        "    parser.add_argument('mode', nargs='?')\n",
        "    parser.add_argument('dataset', nargs='?')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    assert args.mode in ['finetune', 'generate'], \"Mode must be 'finetune' or 'generate'\"\n",
        "\n",
        "    if args.mode == 'finetune':\n",
        "        assert args.dataset is not None, \"You need to provide a dataset.\"\n",
        "\n",
        "        cmd_finetune(dataset=args.dataset, run_name=args.run_name,\n",
        "                     checkpoint_dir=args.checkpoint_dir,\n",
        "                     model_name=args.model_name,\n",
        "                     model_dir=args.model_dir,\n",
        "                     steps=args.steps, restore_from=args.restore_from,\n",
        "                     sample_every=args.sample_every,\n",
        "                     save_every=args.save_every,\n",
        "                     print_every=args.print_every,\n",
        "                     optimizer=args.optimizer,\n",
        "                     overwrite=args.overwrite,\n",
        "                     multi_gpu=args.multi_gpu)\n",
        "    if args.mode == \"generate\":\n",
        "        cmd_generate(nfiles=args.nfiles, nsamples=args.nsamples,\n",
        "                     folder=args.folder, length=args.length,\n",
        "                     temperature=args.temperature, batch_size=args.batch_size,\n",
        "                     prefix=args.prefix, truncate=args.truncate,\n",
        "                     include_prefix=args.include_prefix,\n",
        "                     sample_delim=args.sample_delim, run_name=args.run_name,\n",
        "                     checkpoint_dir=args.checkpoint_dir,\n",
        "                     top_k=args.top_k, top_p=args.top_p, multi_gpu=args.multi_gpu)\n",
        "\n",
        "\n",
        "def cmd_finetune(dataset, run_name, checkpoint_dir, model_name, model_dir, steps,\n",
        "                 restore_from, sample_every,\n",
        "                 save_every, print_every, optimizer, overwrite, multi_gpu):\n",
        "    \"\"\"Wrapper script for finetuning the model via the CLI.\"\"\"\n",
        "\n",
        "    if not is_gpt2_downloaded(model_dir=model_dir, model_name=model_name):\n",
        "        download_gpt2(model_dir=model_dir, model_name=model_name)\n",
        "\n",
        "    sess = start_tf_sess()\n",
        "    finetune(sess, dataset=dataset, run_name=run_name,\n",
        "             checkpoint_dir=checkpoint_dir,\n",
        "             model_name=model_name,\n",
        "             model_dir=model_dir,\n",
        "             steps=steps, restore_from=restore_from,\n",
        "             sample_every=sample_every, save_every=save_every,\n",
        "             print_every=print_every,\n",
        "             optimizer=optimizer,\n",
        "             overwrite=overwrite,\n",
        "             multi_gpu=multi_gpu)\n",
        "\n",
        "\n",
        "def cmd_generate(nfiles, nsamples, folder,\n",
        "                 length, temperature, batch_size,\n",
        "                 prefix, truncate, include_prefix,\n",
        "                 sample_delim, run_name,\n",
        "                 checkpoint_dir,\n",
        "                 top_k, top_p, multi_gpu):\n",
        "    \"\"\"Wrapper script for generating text via the CLI.\n",
        "    The files are generated into a folder, which can be downloaded\n",
        "    recursively by downloading the entire folder.\n",
        "    \"\"\"\n",
        "\n",
        "    sess = start_tf_sess()\n",
        "    load_gpt2(sess, run_name=run_name, checkpoint_dir=checkpoint_dir, multi_gpu=multi_gpu)\n",
        "\n",
        "    try:\n",
        "        os.mkdir(folder)\n",
        "    except:\n",
        "        shutil.rmtree(folder)\n",
        "        os.mkdir(folder)\n",
        "\n",
        "    for _ in trange(nfiles):\n",
        "        gen_file = os.path.join(folder,\n",
        "                    'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow()))\n",
        "\n",
        "        generate_to_file(sess,\n",
        "                         run_name=run_name,\n",
        "                         checkpoint_dir=checkpoint_dir,\n",
        "                         destination_path=gen_file,\n",
        "                         length=length,\n",
        "                         temperature=temperature,\n",
        "                         nsamples=nsamples,\n",
        "                         batch_size=batch_size,\n",
        "                         prefix=prefix,\n",
        "                         truncate=truncate,\n",
        "                         include_prefix=include_prefix,\n",
        "                         sample_delim=sample_delim,\n",
        "                         top_k=top_k,\n",
        "                         top_p=top_p\n",
        "                         )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-40484050338d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_2_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_saving_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_2_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_2_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccumulatingOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_2_simple'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHIw3MJJ7Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}